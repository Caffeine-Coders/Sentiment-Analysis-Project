{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1cpfsgFwtfrr2kbElIzI3PF2gxRWeXr3_",
      "authorship_tag": "ABX9TyMuws0bMCh6Zd3xKLVaCLoj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Caffeine-Coders/Sentiment-Analysis-Project/blob/main/backend/Roberta_custom_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJeYCwSgoBjo",
        "outputId": "4fc32542-dedf-4797-a69b-7b738908cd22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALr-P5kZoDyN",
        "outputId": "155f1c99-344d-4e3e-8f77-8e6c0b8eb619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.7)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "8ypWEqQ4P8Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/DatasetsforSentimentAnalysis/TweetsData.csv', header = None)\n",
        "data[1].fillna(\"random\", inplace=True)\n",
        "data[3].fillna(0, inplace=True)\n",
        "data[3] = data[3].replace(\"neutral\",1)\n",
        "data[3] = data[3].replace(\"negative\",0)\n",
        "data[3] = data[3].replace(\"positive\",2)\n",
        "data = data.drop(0)\n",
        "data[3]= data[3].apply(np.float64)\n",
        "data[3]= data[3].apply(np.int64)\n",
        "# data = data.sample(frac = 1)\n",
        "# data = data.sample(frac = 1)\n",
        "# data = data.sample(frac = 1)\n",
        "data = data.drop(columns = [0,2])\n",
        "columns_names = list(data)\n",
        "data.rename(columns={columns_names[0]:0,\n",
        "                        columns_names[1]:1}, inplace= True)\n",
        "\n",
        "data2 = data.loc[data[1] == 1]\n",
        "data2 = pd.concat([data2, data2])\n",
        "print(type(data2))\n",
        "print(data2)\n",
        "# tweets2 = data[\"text\"].values.tolist()\n",
        "# classifiers2 = data[\"sentiment\"].values.tolist()"
      ],
      "metadata": {
        "id": "KdfN9lv7WFtU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82dfcb8b-cff3-47a9-9ac7-cedd377ffe78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "                                                       0  1\n",
            "1                    I`d have responded, if I were going  1\n",
            "6      http://www.dothebouncy.com/smf - some shameles...  1\n",
            "8                                             Soooo high  1\n",
            "9                                            Both of you  1\n",
            "11      as much as i love to be hopeful, i reckon the...  1\n",
            "...                                                  ... ..\n",
            "27469  few grilled mushrooms and olives, feta cheese ...  1\n",
            "27470              94 more days till BH comes back to LA  1\n",
            "27472  i`m defying gravity. and nobody in alll of oz,...  1\n",
            "27474   in spoke to you yesterday and u didnt respond...  1\n",
            "27481     All this flirting going on - The ATG smiles...  1\n",
            "\n",
            "[22236 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/MyDrive/DatasetsforSentimentAnalysis/sentiment140.csv', \n",
        "                        encoding='latin-1', header = None)\n",
        "  \n",
        "data.drop(data.columns[[1,2, 3,4]], axis=1, inplace=True)\n",
        "data[5].fillna(\"random\", inplace=True)\n",
        "data[0].fillna(0, inplace=True)\n",
        "data[0] = data[0].replace(2,1)\n",
        "data[0] = data[0].replace(4,2)\n",
        "data[0]= data[0].apply(np.int64)\n",
        "columns_names = list(data)\n",
        "print(columns_names)\n",
        "data.rename(columns={columns_names[0]:0,\n",
        "                        columns_names[1]:1}, inplace= True)\n",
        "NUM_SAMPLES = 400000\n",
        "negative_samples = data[data[0]==0][:NUM_SAMPLES]\n",
        "positiv_samples = data[data[0]==2][:NUM_SAMPLES]\n",
        "\n",
        "positiv_samples[0]=[1]*NUM_SAMPLES\n",
        "full_data = pd.concat([negative_samples,  positiv_samples])\n",
        "# data = full_data\n",
        "# data = data.sample(frac = 1)\n",
        "# data = data.sample(frac = 1)\n",
        "# data = data.sample(frac = 1)\n",
        "temp = data[0]\n",
        "data[0] = data[1]\n",
        "data[1] = temp\n",
        "print(type(data))\n",
        "data3 = data\n",
        "print(type(data3))\n",
        "print(data3)\n",
        "# tweets3 = data[5].values.tolist() \n",
        "# classifiers3 = data[0].values.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TzDSbSLXpUJ",
        "outputId": "cff620bc-83f7-490f-a9e3-33b66ff6b248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 5]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "                                                         0  1\n",
            "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...  0\n",
            "1        is upset that he can't update his Facebook by ...  0\n",
            "2        @Kenichan I dived many times for the ball. Man...  0\n",
            "3          my whole body feels itchy and like its on fire   0\n",
            "4        @nationwideclass no, it's not behaving at all....  0\n",
            "...                                                    ... ..\n",
            "1599995  Just woke up. Having no school is the best fee...  2\n",
            "1599996  TheWDB.com - Very cool to hear old Walt interv...  2\n",
            "1599997  Are you ready for your MoJo Makeover? Ask me f...  2\n",
            "1599998  Happy 38th Birthday to my boo of alll time!!! ...  2\n",
            "1599999  happy #charitytuesday @theNSPCC @SparksCharity...  2\n",
            "\n",
            "[1600000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data3.shape)\n",
        "mergedata = pd.concat([data2, data3])\n",
        "print(mergedata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q94-oyI9enK",
        "outputId": "6b943cf1-f284-4fad-b427-0606015fa26c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1600000, 2)\n",
            "                                                         0  1\n",
            "1                      I`d have responded, if I were going  1\n",
            "6        http://www.dothebouncy.com/smf - some shameles...  1\n",
            "8                                               Soooo high  1\n",
            "9                                              Both of you  1\n",
            "11        as much as i love to be hopeful, i reckon the...  1\n",
            "...                                                    ... ..\n",
            "1599995  Just woke up. Having no school is the best fee...  2\n",
            "1599996  TheWDB.com - Very cool to hear old Walt interv...  2\n",
            "1599997  Are you ready for your MoJo Makeover? Ask me f...  2\n",
            "1599998  Happy 38th Birthday to my boo of alll time!!! ...  2\n",
            "1599999  happy #charitytuesday @theNSPCC @SparksCharity...  2\n",
            "\n",
            "[1622236 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import re\n",
        "\n",
        "with open('/content/drive/MyDrive/DatasetsforSentimentAnalysis/book.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "df1 = pd.read_csv(io.StringIO(data), delimiter=\"\\t\", header = None)\n",
        "\n",
        "with open('/content/drive/MyDrive/DatasetsforSentimentAnalysis/kitchen.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "df2 = pd.read_csv(io.StringIO(data), delimiter=\"\\t\", header = None)\n",
        "\n",
        "with open('/content/drive/MyDrive/DatasetsforSentimentAnalysis/dvd.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "df3 = pd.read_csv(io.StringIO(data), delimiter=\"\\t\", header = None)\n",
        "\n",
        "with open('/content/drive/MyDrive/DatasetsforSentimentAnalysis/elec.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "df4 = pd.read_csv(io.StringIO(data), delimiter=\"\\t\", header = None)\n",
        "\n",
        "data1 = []\n",
        "for line in df1[0]:\n",
        "    obtained = line.split(\" #label#:\")\n",
        "    obtained[0] = re.sub(r':\\d+', ' ', obtained[0])\n",
        "    obtained[0] = obtained[0].replace('_', ' ')\n",
        "    data1.append(obtained)\n",
        "\n",
        "data2 = []\n",
        "for line in df2[0]:\n",
        "    obtained = line.split(\" #label#:\")\n",
        "    obtained[0] = re.sub(r':\\d+', ' ', obtained[0])\n",
        "    obtained[0] = obtained[0].replace('_', ' ')\n",
        "    data2.append(obtained)\n",
        "\n",
        "data3 = []\n",
        "for line in df3[0]:\n",
        "    obtained = line.split(\" #label#:\")\n",
        "    obtained[0] = re.sub(r':\\d+', ' ', obtained[0])\n",
        "    obtained[0] = obtained[0].replace('_', ' ')\n",
        "    data3.append(obtained)\n",
        "\n",
        "data4 = []\n",
        "for line in df4[0]:\n",
        "    obtained = line.split(\" #label#:\")\n",
        "    obtained[0] = re.sub(r':\\d+', ' ', obtained[0])\n",
        "    obtained[0] = obtained[0].replace('_', ' ')\n",
        "    data4.append(obtained)\n",
        "\n",
        "df1 = pd.DataFrame(data1)\n",
        "df2 = pd.DataFrame(data2)\n",
        "df3 = pd.DataFrame(data3)\n",
        "df4 = pd.DataFrame(data4)\n",
        "\n",
        "frame = [df1, df2, df3, df4]\n",
        "enddata = pd.concat(frame)\n",
        "enddata[0].fillna(\"random\", inplace=True)\n",
        "enddata[1].fillna('0', inplace=True)\n",
        "enddata[1] = enddata[1].apply(np.float64)\n",
        "enddata[1] = enddata[1].apply(np.int64)\n",
        "enddata[1] = enddata[1].replace(1,0)\n",
        "enddata[1] = enddata[1].replace(2,0)\n",
        "enddata[1] = enddata[1].replace(4,2)\n",
        "enddata[1] = enddata[1].replace(5,2)\n",
        "enddata[1]= enddata[1].apply(np.int64)\n",
        "# enddata = enddata.sample(frac = 1)\n",
        "# enddata = enddata.sample(frac = 1)\n",
        "# enddata = enddata.sample(frac = 1)\n",
        "data4 = enddata\n",
        "print(data4)\n",
        "# tweets4 = enddata[0].values.tolist() \n",
        "# classifiers4 = enddata[1].values.tolist()"
      ],
      "metadata": {
        "id": "S5l56gS_dPC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c04759-6f0f-48f8-ac57-d6c1d393a5cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                      0  1\n",
            "0     is ridiculous  me  who wants  or  of law  or l...  0\n",
            "1     stand  to jackie  reason  whether  are  in  so...  0\n",
            "2     contains numerous  omissio  numerous errors  a...  0\n",
            "3     art  i think  think it  is written  on  by  it...  0\n",
            "4     rainbow.you see  is simply  of simple  artistr...  0\n",
            "...                                                 ... ..\n",
            "1176  you load  whole purpose  look  bottom  intent ...  0\n",
            "1177  i  selling model  the mini   purchase  inserts...  0\n",
            "1178  of  always  time isnt  battery people  product...  0\n",
            "1179  pleased  it the  are  within minutes  than wha...  2\n",
            "1180  is  as the  a phone  nav-traffic  pioneer work...  0\n",
            "\n",
            "[4336 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data4.shape)\n",
        "mergedata = pd.concat([mergedata, data4])\n",
        "print(mergedata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfazLRpu-hBT",
        "outputId": "7e717314-258a-43d8-c726-63c001aa29e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4336, 2)\n",
            "                                                      0  1\n",
            "1                   I`d have responded, if I were going  1\n",
            "6     http://www.dothebouncy.com/smf - some shameles...  1\n",
            "8                                            Soooo high  1\n",
            "9                                           Both of you  1\n",
            "11     as much as i love to be hopeful, i reckon the...  1\n",
            "...                                                 ... ..\n",
            "1176  you load  whole purpose  look  bottom  intent ...  0\n",
            "1177  i  selling model  the mini   purchase  inserts...  0\n",
            "1178  of  always  time isnt  battery people  product...  0\n",
            "1179  pleased  it the  are  within minutes  than wha...  2\n",
            "1180  is  as the  a phone  nav-traffic  pioneer work...  0\n",
            "\n",
            "[1626572 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/DatasetsforSentimentAnalysis/Emojis.csv')\n",
        "data[\"sentiment\"] = data[\"sentiment\"].replace(\"positive\",2)\n",
        "data[\"sentiment\"] = data[\"sentiment\"].replace(\"negative\",0)\n",
        "data[\"sentiment\"] = data[\"sentiment\"].replace(\"neutral\",1)\n",
        "print(data)\n",
        "columns_names = list(data)\n",
        "print(columns_names)\n",
        "data.rename(columns={columns_names[0]:0,\n",
        "                        columns_names[1]:1}, inplace= True)\n",
        "print(data)\n",
        "mergedata = pd.concat([mergedata, data])\n",
        "print(mergedata)\n",
        "# tweets5 = data[\"emoji\"].values.tolist() \n",
        "# classifiers5 = data[\"sentiment\"].values.tolist()\n",
        "\n"
      ],
      "metadata": {
        "id": "9Paojf5JvpE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c965411c-a398-49e2-dd39-aaf09c26fc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    emoji  sentiment\n",
            "0       üòÇ          1\n",
            "1       ‚ù§          2\n",
            "2       ‚ô•          2\n",
            "3       üòç          2\n",
            "4       üò≠          1\n",
            "..    ...        ...\n",
            "964     ‚ûõ          1\n",
            "965     ‚ôù          1\n",
            "966     ‚ùã          1\n",
            "967     ‚úÜ          2\n",
            "968     üìî          1\n",
            "\n",
            "[969 rows x 2 columns]\n",
            "['emoji', 'sentiment']\n",
            "     0  1\n",
            "0    üòÇ  1\n",
            "1    ‚ù§  2\n",
            "2    ‚ô•  2\n",
            "3    üòç  2\n",
            "4    üò≠  1\n",
            "..  .. ..\n",
            "964  ‚ûõ  1\n",
            "965  ‚ôù  1\n",
            "966  ‚ùã  1\n",
            "967  ‚úÜ  2\n",
            "968  üìî  1\n",
            "\n",
            "[969 rows x 2 columns]\n",
            "                                                     0  1\n",
            "1                  I`d have responded, if I were going  1\n",
            "6    http://www.dothebouncy.com/smf - some shameles...  1\n",
            "8                                           Soooo high  1\n",
            "9                                          Both of you  1\n",
            "11    as much as i love to be hopeful, i reckon the...  1\n",
            "..                                                 ... ..\n",
            "964                                                  ‚ûõ  1\n",
            "965                                                  ‚ôù  1\n",
            "966                                                  ‚ùã  1\n",
            "967                                                  ‚úÜ  2\n",
            "968                                                  üìî  1\n",
            "\n",
            "[1627541 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mergedata.shape)\n",
        "mergedata = mergedata.sample(frac = 1)\n",
        "mergedata = mergedata.sample(frac = 1)\n",
        "mergedata = mergedata.sample(frac = 1)\n",
        "mergedata = mergedata.reset_index(drop = True)\n",
        "print(mergedata)\n",
        "tweets5 = mergedata[0].values.tolist() \n",
        "classifiers5 = mergedata[1].values.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtZ9TGRf1-oP",
        "outputId": "3c497c98-37c3-43f6-ebdc-b0a4b6eade7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1627541, 2)\n",
            "                                                         0  1\n",
            "0        still can't get over that huge Superman collec...  0\n",
            "1                              Off to mother's day brunch   2\n",
            "2        wishing i can go to the CARLOS OLIVERO movie n...  0\n",
            "3        gotta back to my (not so) beautiful physics bo...  0\n",
            "4        @TamboManJoe do you know how important is for ...  0\n",
            "...                                                    ... ..\n",
            "1627536  @RandomGuyAndGal I had fun talking w/ you two ...  2\n",
            "1627537       alone in my house..everybody's gone poor me   0\n",
            "1627538  Just one more week...I think I can take it. Bu...  0\n",
            "1627539       Half of the fish are dead or on the verge..   0\n",
            "1627540                          @RetroPop I know! Me too   0\n",
            "\n",
            "[1627541 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo1SAVAUaPRW",
        "outputId": "cc79cf0d-2875-4d3f-8e07-b9631bc00560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.11.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.10.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification, TFTrainer, TFTrainingArguments\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "id": "JO9u4PiIoIj_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8d5308d0-cddc-4efb-cda4-920612f2c1e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\", num_labels=3)\n",
        "print(\"------------------train and test splits --------------------------\")\n",
        "X1_train, X1_test, Y1_train, Y1_test = train_test_split(tweets5, classifiers5, test_size=0.18, random_state=42)\n",
        "# X2_train, X2_test, Y2_train, Y2_test = train_test_split(tweets2, classifiers2, test_size=0.5, random_state=42)\n",
        "# X3_train, X3_test, Y3_train, Y3_test = train_test_split(tweets3, classifiers3, test_size=0.49159, random_state=42)\n",
        "# X4_train, X4_test, Y4_train, Y4test = train_test_split(tweets4, classifiers4, test_size=0.5, random_state=42)\n",
        "# X5_train, X5_test, Y5_train, Y5_test = train_test_split(tweets5, classifiers5, test_size=0.05, random_state=42)\n",
        "# print(len(X1_train))\n",
        "# print(len(X2_train))\n",
        "# print(len(X3_train))\n",
        "# print(len(X4_train))\n",
        "# # print(len(X5_train))\n",
        "# X1_train.extend(X2_train)\n",
        "# X1_train.extend(X3_train)\n",
        "# X1_train.extend(X4_train)\n",
        "# # X1_train.extend(X5_train)\n",
        "# print(len(X1_train))\n",
        "# Y1_train.extend(Y2_train)\n",
        "# Y1_train.extend(Y3_train)\n",
        "# Y1_train.extend(Y4_train)\n",
        "# Y1_train.extend(Y5_train)\n",
        "print(len(Y1_train), len(X1_train))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X1_train)\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices(X1_test)"
      ],
      "metadata": {
        "id": "zMroiQ93oKBH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e5b4d1-93dd-4622-e7d1-62eada9519aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------train and test splits --------------------------\n",
            "1334583 1334583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-------------------tokenizing------------------------------\")\n",
        "train_encodings1 = tokenizer(X1_train, padding = \"max_length\", truncation = True ,return_tensors='np')"
      ],
      "metadata": {
        "id": "DwHrK1Jly-TG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91af7606-5649-403b-ea54-1cbc1917a886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------tokenizing------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = mergedata.map(train_encodings1)\n",
        "model = TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)\n",
        "tf_dataset = model.prepare_tf_dataset(dataset, batch_size = 128, shuffle = True, tokenizer = tokenizer)\n",
        "train_labels1 = np.array(Y1_train).astype(np.int64)\n",
        "test_labels1 = np.array(Y1_test).astype(np.int64)\n",
        "opt = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)  # Transformers like lower learning rates\n",
        "model.compile(optimizer=opt, metrics = [tf.keras.metrics.SparseCategoricalAccuracy(), tf.keras.metrics.Accuracy(), tf.keras.metrics.TopKCategoricalAccuracy(k = 3)])\n",
        "early_stop = EarlyStopping(monitor = 'loss', patience = 2)"
      ],
      "metadata": {
        "id": "1lkMJGkeoMTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DFQKrmqEfZ9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------Training happens here -----------------------------"
      ],
      "metadata": {
        "id": "V5JTdGwHHyE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dict(train_encodings1), train_labels1, epochs=150, callbacks = [early_stop], validatation_data = (dataset_test, test_labels1))\n",
        "model.save_pretrained('/content/customRobertaModel')"
      ],
      "metadata": {
        "id": "2Z4_u6SNKH4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model is trained\")"
      ],
      "metadata": {
        "id": "cVkdAZOsoBli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/customRobertaModel.zip /content/customRobertaModel/"
      ],
      "metadata": {
        "id": "D0Kjz1nl1lPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/customRobertaModel.zip\")"
      ],
      "metadata": {
        "id": "REuMyOrO13io"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}